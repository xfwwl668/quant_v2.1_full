"""
src/data/collector.py
=====================
Phase 6 â€” TDX å¹¶è¡Œæ•°æ®é‡‡é›†å™¨ï¼ˆv2.0.1 å·¥ä¸šçº§é‡æ„ç‰ˆï¼‰

èŒè´£ï¼š
  1. å…¨å¸‚åœº A è‚¡æ—¥çº¿æ•°æ®é‡‡é›†ï¼ˆpytdx â†’ parquet å…¨é“¾è·¯ï¼‰
  2. é•¿è¿æ¥æ±  + è´Ÿè½½å‡è¡¡ï¼ˆ15-30 çº¿ç¨‹é«˜å¹¶å‘ï¼‰
  3. å¢é‡æ›´æ–° + å‰å¤æƒå¤„ç†
  4. å¥‘çº¦å¼ ETL Pipeline: Download â†’ Validate â†’ Sanitize â†’ Save

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
æ¶æ„å¯¹é½ï¼ˆv2.0.1 æ ‡å‡†ï¼‰
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. **Path Hijackingï¼ˆå¼ºå¥‘çº¦ï¼‰**
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   storage.parquet_dir = base_dir/market_data/parquet/daily
   
   æœ¬æ¨¡å— MUST NOT é‡æ–°å®šä¹‰è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨ï¼š
   ```python
   self.storage.parquet_dir  # å”¯ä¸€è·¯å¾„æº
   ```

2. **å­—æ®µå¥‘çº¦ï¼ˆä¸¥æ ¼å¯¹é½ types.pyï¼‰**
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   DataSanitizer è¦æ±‚å­—æ®µï¼š
     - open, high, low, close, volume (float64)
     - date (datetime64 index)
   
   TDX åŸå§‹å­—æ®µï¼š
     - vol â†’ volume (å¿…é¡»ç«‹å³ rename)
     - amount (ä¿ç•™ï¼Œå¯é€‰)
   
   rename å¿…é¡»åœ¨ sanitize ä¹‹å‰å®Œæˆï¼Œå¦åˆ™ DataSanitizer æŠ¥é”™ã€‚

3. **å¹¶å‘å®‰å…¨ï¼ˆCRITICAL FIXï¼‰**
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   å²å‰ç‰ˆæœ¬é—®é¢˜ï¼š
     - æ—¥å¿—ç«æ€å¯¼è‡´æ­»é”ï¼ˆ15 çº¿ç¨‹åœºæ™¯ï¼‰
     - as_completed æœªå®æ—¶åˆ·æ–°è¿›åº¦
   
   v2.0.1 ä¿®å¤ï¼š
     - ä½¿ç”¨ queue.Queue å¼‚æ­¥æ—¥å¿—ï¼ˆLogger çº¿ç¨‹ç‹¬ç«‹ï¼‰
     - as_completed æ¯ç¬”å®Œæˆç«‹å³å›è°ƒ
     - æ‰¹é‡èšåˆæ—¥å¿—ï¼ˆå‡å°‘ I/O ç«äº‰ï¼‰

4. **Numba å®‰å…¨æ€§ï¼ˆå¯¹æ¥ rsrs.pyï¼‰**
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   DataSanitizer.sanitize_dataframe() ä¿è¯ï¼š
     - NaN ä¿®å¤ï¼ˆMAD ä¸­å€¼å¡«å……ï¼‰
     - ä»·æ ¼å¼‚å¸¸å€¼ä¿®å¤ï¼ˆforward fillï¼‰
     - volume < 100 ä¿®å¤ï¼ˆä¸­å€¼æ›¿æ¢ï¼‰
     - æ•°æ®ç±»å‹å¼ºåˆ¶ float64ï¼ˆNumba nopython æ¨¡å¼è¦æ±‚ï¼‰

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
æ€§èƒ½ç›®æ ‡
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

- èŠ‚ç‚¹æµ‹é€Ÿ: < 5sï¼ˆ30 å¹¶å‘ï¼‰
- ä¸‹è½½é€Ÿåº¦: 15-20 stocks/sï¼ˆ15 çº¿ç¨‹ï¼‰
- å…¨å¸‚åœºé‡‡é›†: 5500 stocks < 6 åˆ†é’Ÿ

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

from __future__ import annotations

import logging
import queue
import sys
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from threading import Lock, local
from typing import Any, Callable, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

# pytdx ä¾èµ–æ£€æŸ¥
try:
    from pytdx.hq import TdxHq_API
    PYTDX_AVAILABLE = True
except ImportError:
    PYTDX_AVAILABLE = False
    import warnings
    warnings.warn(
        "pytdx æœªå®‰è£…ï¼Œæ•°æ®é‡‡é›†åŠŸèƒ½ä¸å¯ç”¨ã€‚å®‰è£…: pip install pytdx",
        RuntimeWarning
    )

# å¯¼å…¥å·²ä¿®å¤çš„æ¨¡å—
from .storage import ColumnarStorageManager
from .sanitizer import DataSanitizer


# ============================================================================
# Part 1: å¹¶å‘å®‰å…¨æ—¥å¿—ç³»ç»Ÿï¼ˆCRITICAL FIXï¼‰
# ============================================================================

class AsyncLogHandler:
    """
    å¼‚æ­¥æ—¥å¿—å¤„ç†å™¨ï¼ˆè§£å†³å¤šçº¿ç¨‹æ—¥å¿—æ­»é”ï¼‰ã€‚
    
    é—®é¢˜æ ¹æºï¼š
      - logging æ¨¡å—åœ¨é«˜å¹¶å‘ä¸‹å­˜åœ¨ GIL ç«äº‰
      - 15 çº¿ç¨‹åŒæ—¶è°ƒç”¨ logger.info() å¯¼è‡´ I/O é˜»å¡
    
    è§£å†³æ–¹æ¡ˆï¼š
      - ä¸»çº¿ç¨‹å¯åŠ¨ç‹¬ç«‹ Logger çº¿ç¨‹
      - å·¥ä½œçº¿ç¨‹é€šè¿‡ queue.Queue å¼‚æ­¥å‘é€æ—¥å¿—
      - Logger çº¿ç¨‹è´Ÿè´£æ‰€æœ‰ I/O æ“ä½œ
    """
    
    def __init__(self, logger: logging.Logger, max_queue_size: int = 10000):
        self.logger = logger
        self.log_queue: queue.Queue = queue.Queue(maxsize=max_queue_size)
        self._stop_event = threading.Event()
        self._thread: Optional[threading.Thread] = None
        self._is_running = False
    
    def start(self) -> None:
        """å¯åŠ¨å¼‚æ­¥æ—¥å¿—çº¿ç¨‹"""
        if self._is_running:
            return
        
        self._stop_event.clear()
        self._thread = threading.Thread(
            target=self._log_worker,
            name="AsyncLogger",
            daemon=True
        )
        self._thread.start()
        self._is_running = True
    
    def stop(self, timeout: float = 5.0) -> None:
        """åœæ­¢å¼‚æ­¥æ—¥å¿—çº¿ç¨‹"""
        if not self._is_running:
            return
        
        self._stop_event.set()
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=timeout)
        self._is_running = False
    
    def _log_worker(self) -> None:
        """æ—¥å¿—å·¥ä½œçº¿ç¨‹ï¼ˆç‹¬ç«‹çº¿ç¨‹ï¼Œæ—  GIL ç«äº‰ï¼‰"""
        while not self._stop_event.is_set():
            try:
                # 0.1s è¶…æ—¶ï¼Œé¿å…é€€å‡ºæ—¶å¡ä½
                level, msg = self.log_queue.get(timeout=0.1)
                
                if level == logging.DEBUG:
                    self.logger.debug(msg)
                elif level == logging.INFO:
                    self.logger.info(msg)
                elif level == logging.WARNING:
                    self.logger.warning(msg)
                elif level == logging.ERROR:
                    self.logger.error(msg)
                
                self.log_queue.task_done()
            except queue.Empty:
                continue
            except Exception as e:
                # é™çº§åˆ°åŒæ­¥æ—¥å¿—ï¼ˆé¿å…æ—¥å¿—ç³»ç»Ÿæœ¬èº«å´©æºƒï¼‰
                print(f"[AsyncLogger ERROR] {e}", file=sys.stderr)
    
    def log(self, level: int, msg: str) -> None:
        """å¼‚æ­¥å‘é€æ—¥å¿—ï¼ˆéé˜»å¡ï¼‰"""
        try:
            self.log_queue.put_nowait((level, msg))
        except queue.Full:
            # é˜Ÿåˆ—æ»¡æ—¶é™çº§åˆ°åŒæ­¥æ—¥å¿—
            self.logger.log(level, msg)
    
    def debug(self, msg: str) -> None:
        self.log(logging.DEBUG, msg)
    
    def info(self, msg: str) -> None:
        self.log(logging.INFO, msg)
    
    def warning(self, msg: str) -> None:
        self.log(logging.WARNING, msg)
    
    def error(self, msg: str) -> None:
        self.log(logging.ERROR, msg)


# ============================================================================
# Part 2: æ•°æ®ç»“æ„å®šä¹‰
# ============================================================================

@dataclass
class TdxNode:
    """TDX æœåŠ¡å™¨èŠ‚ç‚¹ï¼ˆimmutable for thread safetyï¼‰"""
    name: str
    host: str
    port: int
    latency: float = float('inf')
    is_available: bool = False
    fail_count: int = 0


@dataclass
class DownloadResult:
    """ä¸‹è½½ç»“æœï¼ˆå¥‘çº¦è¾“å‡ºï¼‰"""
    code: str
    success: bool
    records: int = 0
    message: str = ""
    elapsed_time: float = 0.0
    
    def __str__(self) -> str:
        status = "âœ“" if self.success else "âœ—"
        if self.success and self.records > 0:
            return f"{status} {self.code} | {self.records} records | {self.elapsed_time:.2f}s"
        else:
            return f"{status} {self.code} | {self.message}"


# ============================================================================
# Part 3: TDX èŠ‚ç‚¹ç®¡ç†å™¨
# ============================================================================

class TdxNodeManager:
    """
    TDX èŠ‚ç‚¹ç®¡ç†å™¨ï¼ˆè´Ÿè½½å‡è¡¡ + æ•…éšœè½¬ç§»ï¼‰ã€‚
    
    å…³é”®ç®—æ³•ï¼š
      - è½®è¯¢è°ƒåº¦: worker_id % len(available_nodes)
      - æ•…éšœè½¬ç§»: fail_count >= 5 è‡ªåŠ¨ç¦ç”¨
      - è‡ªåŠ¨æ¢å¤: æˆåŠŸæ—¶é€’å‡ fail_count
    """
    
    # ç²¾é€‰é«˜å¯ç”¨èŠ‚ç‚¹ï¼ˆç»ç”Ÿäº§éªŒè¯ï¼‰
    DEFAULT_NODES: List[Tuple[str, str, int]] = [
        # ä¸€çº¿åˆ¸å•†ï¼ˆæœ€ç¨³å®šï¼‰
        ("æ‹›å•†è¯åˆ¸æ·±åœ³", "119.147.212.81", 7709),
        ("åæ³°è¯åˆ¸ä¸Šæµ·", "180.153.39.51", 7709),
        ("å›½ä¿¡è¯åˆ¸æ·±åœ³", "120.79.60.82", 7709),
        ("ä¸­ä¿¡è¯åˆ¸ä¸Šæµ·", "101.227.73.20", 7709),
        ("é“¶æ²³è¯åˆ¸åŒ—äº¬", "106.120.74.86", 7709),
        ("å¹¿å‘è¯åˆ¸æ·±åœ³", "14.17.75.71", 7709),
        ("å›½æ³°å›å®‰ä¸Šæµ·", "180.153.18.170", 7709),
        ("æµ·é€šè¯åˆ¸æ­å·", "115.238.56.198", 7709),
        # é€šè¾¾ä¿¡ä¸»ç«™
        ("é€šè¾¾ä¿¡ä¸»ç«™1", "110.41.147.114", 7709),
        ("é€šè¾¾ä¿¡ä¸»ç«™2", "221.194.181.176", 7709),
        ("é€šè¾¾ä¿¡ä¸»ç«™3", "59.175.238.38", 7709),
        ("é€šè¾¾ä¿¡é«˜å¸¦A", "112.74.214.43", 7721),
        ("é€šè¾¾ä¿¡é«˜å¸¦B", "120.24.149.28", 7721),
        # å¤‡ç”¨èŠ‚ç‚¹
        ("ä¸œæ–¹è´¢å¯Œä¸Šæµ·", "183.136.120.48", 7709),
        ("å¹³å®‰è¯åˆ¸æ·±åœ³", "113.105.142.136", 7709),
    ]
    
    def __init__(self, timeout: float = 3.0, max_fail_count: int = 5):
        self.timeout = timeout
        self.max_fail_count = max_fail_count
        self.nodes: List[TdxNode] = [
            TdxNode(name=name, host=host, port=port)
            for name, host, port in self.DEFAULT_NODES
        ]
        self._lock = Lock()
    
    def ping_node(self, node: TdxNode) -> TdxNode:
        """æµ‹è¯•å•ä¸ªèŠ‚ç‚¹å»¶è¿Ÿ"""
        if not PYTDX_AVAILABLE:
            return node
        
        api = TdxHq_API()
        try:
            start = time.perf_counter()
            if api.connect(node.host, node.port, time_out=self.timeout):
                count = api.get_security_count(0)
                if count and count > 0:
                    node.latency = (time.perf_counter() - start) * 1000
                    node.is_available = True
                    node.fail_count = 0
        except Exception:
            node.is_available = False
        finally:
            try:
                api.disconnect()
            except Exception as e:
                pass
        
        return node
    
    def test_all_nodes(
        self,
        max_workers: int = 30,
        async_logger: Optional[AsyncLogHandler] = None
    ) -> List[TdxNode]:
        """å¹¶è¡Œæµ‹è¯•æ‰€æœ‰èŠ‚ç‚¹ï¼ˆ30 å¹¶å‘ < 5sï¼‰"""
        logger_obj = async_logger if async_logger else logging.getLogger(__name__)
        
        logger_obj.info("ğŸ” å¼€å§‹æµ‹è¯• TDX èŠ‚ç‚¹...")
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=min(len(self.nodes), max_workers)) as executor:
            futures = {executor.submit(self.ping_node, node): node for node in self.nodes}
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception:
                    pass
        
        available = sorted(
            [n for n in self.nodes if n.is_available],
            key=lambda x: x.latency
        )
        
        elapsed = time.time() - start_time
        logger_obj.info(
            f"âœ… èŠ‚ç‚¹æµ‹è¯•å®Œæˆ: {len(available)}/{len(self.nodes)} å¯ç”¨ | "
            f"è€—æ—¶ {elapsed:.2f}s"
        )
        
        if available:
            top5 = ", ".join(f"{n.name}({n.latency:.0f}ms)" for n in available[:5])
            logger_obj.info(f"ğŸš€ æœ€å¿«èŠ‚ç‚¹: {top5}")
        
        return available
    
    def get_node_by_index(self, index: int) -> Optional[TdxNode]:
        """è½®è¯¢è°ƒåº¦ï¼ˆè´Ÿè½½å‡è¡¡æ ¸å¿ƒï¼‰"""
        with self._lock:
            available = [
                n for n in self.nodes
                if n.is_available and n.fail_count < self.max_fail_count
            ]
            if not available:
                return None
            return available[index % len(available)]
    
    def get_available_count(self) -> int:
        """è·å–å¯ç”¨èŠ‚ç‚¹æ•°"""
        with self._lock:
            return len([
                n for n in self.nodes
                if n.is_available and n.fail_count < self.max_fail_count
            ])
    
    def report_failure(self, node: TdxNode) -> None:
        """æŠ¥å‘ŠèŠ‚ç‚¹å¤±è´¥ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self._lock:
            node.fail_count += 1
            if node.fail_count >= self.max_fail_count:
                node.is_available = False
    
    def report_success(self, node: TdxNode) -> None:
        """æŠ¥å‘ŠèŠ‚ç‚¹æˆåŠŸï¼ˆè‡ªåŠ¨æ¢å¤ï¼‰"""
        with self._lock:
            if node.fail_count > 0:
                node.fail_count = max(0, node.fail_count - 1)


# ============================================================================
# Part 4: å‰å¤æƒå¤„ç†å™¨
# ============================================================================

class ForwardAdjustmentProcessor:
    """å‰å¤æƒå¤„ç†å™¨ï¼ˆé™æ€å·¥å…·ç±»ï¼‰"""
    
    @staticmethod
    def apply_forward_adjust(
        df: pd.DataFrame,
        xdxr_data: List[Dict]
    ) -> pd.DataFrame:
        """
        åº”ç”¨å‰å¤æƒã€‚
        
        ç®—æ³•ï¼š
          - ä»æœ€æ–°é™¤æƒæ—¥å‘å†å²å›æº¯
          - factor = 1 + é€è½¬è‚¡æ¯”ä¾‹ + é…è‚¡æ¯”ä¾‹
          - adjusted_price = (åŸä»· - åˆ†çº¢) / factor
        """
        if not xdxr_data or df.empty:
            return df
        
        try:
            xdxr_df = pd.DataFrame(xdxr_data)
            xdxr_df = xdxr_df[xdxr_df['category'] == 1].copy()
            
            if xdxr_df.empty:
                return df
            
            # è§£æé™¤æƒæ—¥æœŸ
            xdxr_df['date'] = pd.to_datetime(
                xdxr_df['year'].astype(str) + '-' +
                xdxr_df['month'].astype(str).str.zfill(2) + '-' +
                xdxr_df['day'].astype(str).str.zfill(2)
            )
            xdxr_df = xdxr_df.sort_values('date')
            
            result = df.copy()
            price_cols = ['open', 'high', 'low', 'close']
            
            # ä»æœ€æ–°é™¤æƒæ—¥å‘å†å²å›æº¯
            for _, row in xdxr_df.iloc[::-1].iterrows():
                ex_date = row['date']
                songzhuangu = float(row.get('songzhuangu', 0) or 0) / 10
                peigu = float(row.get('peigu', 0) or 0) / 10
                fenhong = float(row.get('fenhong', 0) or 0) / 10
                factor = 1 + songzhuangu + peigu
                
                if factor > 0:
                    mask = result.index < ex_date
                    for col in price_cols:
                        if col in result.columns:
                            result.loc[mask, col] = (
                                result.loc[mask, col] - fenhong
                            ) / factor
            
            # ä»·æ ¼ä¸‹é™ä¿æŠ¤
            for col in price_cols:
                if col in result.columns:
                    result[col] = result[col].clip(lower=0.01)
            
            return result
        
        except Exception as e:
            logging.getLogger(__name__).debug(f"å‰å¤æƒå¤„ç†å¼‚å¸¸: {e}")
            return df


# ============================================================================
# Part 5: çº¿ç¨‹æœ¬åœ° API ç®¡ç†å™¨ï¼ˆé•¿è¿æ¥æ± ï¼‰
# ============================================================================

class ThreadLocalAPI:
    """çº¿ç¨‹æœ¬åœ° API ç®¡ç†å™¨ï¼ˆé•¿è¿æ¥å¤ç”¨ï¼‰"""
    
    def __init__(self):
        self._local = local()
    
    def get_api(self) -> Optional[TdxHq_API]:
        return getattr(self._local, 'api', None)
    
    def get_node(self) -> Optional[TdxNode]:
        return getattr(self._local, 'node', None)
    
    def set_connection(self, api: TdxHq_API, node: TdxNode) -> None:
        self._local.api = api
        self._local.node = node
    
    def clear(self) -> None:
        if hasattr(self._local, 'api'):
            try:
                self._local.api.disconnect()
            except Exception as e:
                pass
            delattr(self._local, 'api')
        if hasattr(self._local, 'node'):
            delattr(self._local, 'node')


# ============================================================================
# Part 6: TDX å¹¶è¡Œä¸‹è½½å™¨ï¼ˆä¸»ç±»ï¼‰
# ============================================================================

class TdxParallelDownloader:
    """
    TDX å¹¶è¡Œæ•°æ®ä¸‹è½½å™¨ v2.0.1ï¼ˆå·¥ä¸šçº§é‡æ„ç‰ˆï¼‰ã€‚
    
    æ¶æ„å¯¹é½ï¼š
      - Path: ä½¿ç”¨ storage.parquet_dirï¼ˆå¼ºå¥‘çº¦ï¼‰
      - ETL: Download â†’ Validate â†’ Sanitize â†’ Save
      - å¹¶å‘: AsyncLogger + as_completed å®æ—¶è¿›åº¦
      - å®‰å…¨: DataSanitizer ä¿è¯ Numba å…¼å®¹æ€§
    
    æ€§èƒ½ç›®æ ‡ï¼š
      - 15-20 stocks/s (15 çº¿ç¨‹)
      - 5500 stocks < 6 åˆ†é’Ÿ
    """
    
    # æœ‰æ•ˆè‚¡ç¥¨ä»£ç å‰ç¼€ï¼ˆå¥‘çº¦ï¼‰
    VALID_PREFIXES_SZ = ('00', '30', '8', '43')
    VALID_PREFIXES_SH = ('60', '688')
    
    def __init__(
        self,
        storage_manager: ColumnarStorageManager,
        data_sanitizer: DataSanitizer,
        max_workers: int = 15,
        timeout: float = 5.0,
        enable_adjust: bool = True,
        enable_async_log: bool = True
    ):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨ã€‚
        
        Args:
            storage_manager: å­˜å‚¨ç®¡ç†å™¨ï¼ˆè·¯å¾„å¥‘çº¦æ¥æºï¼‰
            data_sanitizer: æ•°æ®æ¸…æ´—å™¨ï¼ˆNumba å®‰å…¨æ€§ä¿è¯ï¼‰
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼ˆå»ºè®® 15-30ï¼‰
            timeout: è¿æ¥è¶…æ—¶ï¼ˆç§’ï¼‰
            enable_adjust: æ˜¯å¦å¯ç”¨å‰å¤æƒ
            enable_async_log: æ˜¯å¦å¯ç”¨å¼‚æ­¥æ—¥å¿—ï¼ˆé«˜å¹¶å‘å¿…é¡»ï¼‰
        """
        if not PYTDX_AVAILABLE:
            raise RuntimeError("pytdx æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨æ•°æ®é‡‡é›†åŠŸèƒ½")
        
        self.storage = storage_manager
        self.sanitizer = data_sanitizer
        self.max_workers = max_workers
        self.timeout = timeout
        self.enable_adjust = enable_adjust
        
        # ================================================================
        # ã€å¥‘çº¦ã€‘Path Hijacking - ç›´æ¥ä½¿ç”¨ storage.parquet_dir
        # ================================================================
        self.parquet_dir = self.storage.parquet_dir
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.node_manager = TdxNodeManager(timeout=timeout)
        self.adjust_processor = ForwardAdjustmentProcessor()
        self.thread_local_api = ThreadLocalAPI()
        
        # ã€CRITICAL FIXã€‘å¼‚æ­¥æ—¥å¿—ç³»ç»Ÿ
        self._base_logger = logging.getLogger(__name__)
        if enable_async_log:
            self.logger = AsyncLogHandler(self._base_logger)
            self.logger.start()
        else:
            self.logger = self._base_logger  # type: ignore
        
        # ç»Ÿè®¡ä¿¡æ¯ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        self._stats: Dict[str, Any] = {}
        self._stats_lock = Lock()
        
        self.logger.info(f"ğŸ“‚ æ•°æ®å­˜å‚¨è·¯å¾„: {self.parquet_dir}")
    
    def __del__(self):
        """ææ„æ—¶åœæ­¢å¼‚æ­¥æ—¥å¿—"""
        if isinstance(self.logger, AsyncLogHandler):
            self.logger.stop()
    
    def test_nodes(self) -> List[TdxNode]:
        """æµ‹è¯•æ‰€æœ‰èŠ‚ç‚¹"""
        return self.node_manager.test_all_nodes(
            max_workers=30,
            async_logger=self.logger if isinstance(self.logger, AsyncLogHandler) else None
        )
    
    def get_all_stock_codes(self) -> List[Tuple[int, str]]:
        """
        è·å–å…¨å¸‚åœº A è‚¡ä»£ç åˆ—è¡¨ã€‚
        
        å¥‘çº¦ï¼š
          - æ·±åœ³(0): 00/30/8/43
          - ä¸Šæµ·(1): 60/688
        """
        node = self.node_manager.get_node_by_index(0)
        if node is None:
            self.test_nodes()
            node = self.node_manager.get_node_by_index(0)
        if node is None:
            raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„ TDX èŠ‚ç‚¹")
        
        self.logger.info("ğŸ“‹ æ­£åœ¨è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨...")
        
        all_stocks = []
        api = TdxHq_API()
        
        try:
            if not api.connect(node.host, node.port, time_out=self.timeout):
                raise ConnectionError(f"æ— æ³•è¿æ¥åˆ°èŠ‚ç‚¹ {node.name}")
            
            for market in [0, 1]:
                count = api.get_security_count(market)
                if not count or count <= 0:
                    continue
                
                for start in range(0, count, 1000):
                    stocks = api.get_security_list(market, start)
                    if not stocks:
                        continue
                    
                    for stock in stocks:
                        code = stock['code']
                        
                        if market == 0:
                            if any(code.startswith(p) for p in self.VALID_PREFIXES_SZ):
                                all_stocks.append((market, code))
                        else:
                            if any(code.startswith(p) for p in self.VALID_PREFIXES_SH):
                                all_stocks.append((market, code))
        
        finally:
            try:
                api.disconnect()
            except Exception as e:
                pass
        
        self.logger.info(
            f"âœ… è·å–åˆ° {len(all_stocks)} åª A è‚¡ | "
            f"æ·±åœ³: {sum(1 for m, _ in all_stocks if m == 0)} | "
            f"ä¸Šæµ·: {sum(1 for m, _ in all_stocks if m == 1)}"
        )
        
        return all_stocks
    
    def _ensure_connection(
        self,
        worker_id: int
    ) -> Tuple[Optional[TdxHq_API], Optional[TdxNode]]:
        """ç¡®ä¿å½“å‰çº¿ç¨‹æœ‰å¯ç”¨è¿æ¥ï¼ˆé•¿è¿æ¥å¤ç”¨ï¼‰"""
        api = self.thread_local_api.get_api()
        node = self.thread_local_api.get_node()
        
        # éªŒè¯ç°æœ‰è¿æ¥
        if api and node:
            try:
                if api.get_security_count(0):
                    return api, node
            except Exception as e:
                pass
            
            self.thread_local_api.clear()
        
        # è½®è¯¢åˆ†é…èŠ‚ç‚¹
        node = self.node_manager.get_node_by_index(worker_id)
        if not node:
            return None, None
        
        # å»ºç«‹æ–°è¿æ¥
        api = TdxHq_API()
        try:
            if api.connect(node.host, node.port, time_out=self.timeout):
                self.thread_local_api.set_connection(api, node)
                return api, node
        except Exception:
            self.node_manager.report_failure(node)
        
        return None, None
    
    def _download_stock_data(
        self,
        api: TdxHq_API,
        market: int,
        code: str,
        start_date: Optional[datetime] = None
    ) -> Tuple[Optional[pd.DataFrame], List[Dict]]:
        """
        ä¸‹è½½å•åªè‚¡ç¥¨æ•°æ®ã€‚
        
        å¥‘çº¦ï¼š
          - ä½¿ç”¨ count=800 æ‰¹é‡è·å–
          - ç«‹å³ rename vol â†’ volume
          - è¿”å› DataFrame + é™¤æƒæ•°æ®
        """
        all_data = []
        xdxr_data = []
        
        # æ‰¹é‡è·å–ï¼ˆ800 æœ€ä¼˜ï¼‰
        start = 0
        while True:
            data = api.get_security_bars(
                category=9,
                market=market,
                code=code,
                start=start,
                count=800
            )
            
            if not data:
                break
            
            all_data.extend(data)
            start += 800
            
            if len(data) < 800:
                break
        
        # è·å–é™¤æƒé™¤æ¯
        if self.enable_adjust:
            xdxr_data = api.get_xdxr_info(market, code) or []
        
        if not all_data:
            return None, xdxr_data
        
        # è½¬æ¢ä¸º DataFrame
        df = pd.DataFrame(all_data)
        df['date'] = pd.to_datetime(df['datetime'].str[:10])
        
        # ================================================================
        # ã€å¥‘çº¦ã€‘CRITICAL: ç«‹å³ rename vol â†’ volume
        # ================================================================
        if 'vol' in df.columns:
            df = df.rename(columns={'vol': 'volume'})
        
        # é€‰æ‹©éœ€è¦çš„åˆ—ï¼ˆå¥‘çº¦å­—æ®µï¼‰
        columns = ['date', 'open', 'high', 'low', 'close', 'volume', 'amount']
        df = df[[c for c in columns if c in df.columns]]
        df = df.set_index('date').sort_index()
        df = df[~df.index.duplicated(keep='last')]
        
        # åº”ç”¨å‰å¤æƒ
        if self.enable_adjust and xdxr_data:
            df = self.adjust_processor.apply_forward_adjust(df, xdxr_data)
        
        # å¢é‡è¿‡æ»¤
        if start_date:
            df = df[df.index >= pd.Timestamp(start_date)]
        
        return df, xdxr_data
    
    def _download_worker(
        self,
        tasks: List[Tuple[int, str]],
        worker_id: int
    ) -> List[DownloadResult]:
        """
        å·¥ä½œçº¿ç¨‹ï¼ˆå¤„ç†ä¸€æ‰¹è‚¡ç¥¨ï¼‰ã€‚
        
        ETL Pipeline:
          1. Downloadï¼ˆTDX APIï¼‰
          2. Validateï¼ˆå­—æ®µæ£€æŸ¥ï¼‰
          3. Sanitizeï¼ˆDataSanitizerï¼‰
          4. Saveï¼ˆstorage.save_stock_dataï¼‰
        """
        results = []
        
        try:
            for market, code in tasks:
                start_time = time.time()
                
                try:
                    # ç¡®ä¿è¿æ¥
                    api, node = self._ensure_connection(worker_id)
                    if not api or not node:
                        results.append(DownloadResult(
                            code=code,
                            success=False,
                            message="æ— å¯ç”¨è¿æ¥",
                            elapsed_time=time.time() - start_time
                        ))
                        continue
                    
                    # æ£€æŸ¥å¢é‡æ›´æ–°
                    last_date = self._get_last_local_date(code)
                    start_date = None
                    is_incremental = False
                    
                    if last_date:
                        start_date = last_date + timedelta(days=1)
                        if start_date.date() >= datetime.now().date():
                            results.append(DownloadResult(
                                code=code,
                                success=True,
                                records=0,
                                message="å·²æ˜¯æœ€æ–°",
                                elapsed_time=time.time() - start_time
                            ))
                            continue
                        is_incremental = True
                    
                    # ========================================================
                    # ETL Pipeline
                    # ========================================================
                    
                    # Step 1: Download
                    df, _ = self._download_stock_data(api, market, code, start_date)
                    
                    if df is None or df.empty:
                        self.node_manager.report_success(node)
                        results.append(DownloadResult(
                            code=code,
                            success=True,
                            records=0,
                            message="æ— æ–°æ•°æ®" if is_incremental else "æ— æ•°æ®",
                            elapsed_time=time.time() - start_time
                        ))
                        continue
                    
                    # Step 2: Validateï¼ˆå­—æ®µå¥‘çº¦æ£€æŸ¥ï¼‰
                    required = ['open', 'high', 'low', 'close', 'volume']
                    missing = [c for c in required if c not in df.columns]
                    if missing:
                        raise ValueError(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing}")
                    
                    # Step 3: Sanitizeï¼ˆNumba å®‰å…¨æ€§ä¿è¯ï¼‰
                    df = self.sanitizer.sanitize_dataframe(df)
                    
                    # Step 4: Save
                    self.storage.save_stock_data(code, df)
                    
                    self.node_manager.report_success(node)
                    results.append(DownloadResult(
                        code=code,
                        success=True,
                        records=len(df),
                        message="æˆåŠŸ",
                        elapsed_time=time.time() - start_time
                    ))
                
                except Exception as e:
                    node = self.thread_local_api.get_node()
                    if node:
                        self.node_manager.report_failure(node)
                    
                    results.append(DownloadResult(
                        code=code,
                        success=False,
                        message=str(e)[:50],
                        elapsed_time=time.time() - start_time
                    ))
                    
                    # æ¸…ç†æ–­å¼€çš„è¿æ¥
                    self.thread_local_api.clear()
        
        finally:
            self.thread_local_api.clear()
        
        return results
    
    def _get_last_local_date(self, code: str) -> Optional[datetime]:
        """è·å–æœ¬åœ°å­˜å‚¨çš„æœ€åæ—¥æœŸ"""
        try:
            file_path = self.parquet_dir / f"{code}.parquet"
            if file_path.exists():
                df = pd.read_parquet(file_path)
                if not df.empty:
                    if 'date' in df.columns:
                        return pd.to_datetime(df['date'].max())
                    elif df.index.name == 'date':
                        return pd.to_datetime(df.index.max())
        except Exception:
            pass
        return None
    
    def download_all_stocks(
        self,
        stock_list: Optional[List[Tuple[int, str]]] = None,
        progress_callback: Optional[Callable[[int, int, str], None]] = None,
        batch_log_size: int = 50
    ) -> Dict[str, Any]:
        """
        å¹¶è¡Œä¸‹è½½å…¨å¸‚åœºè‚¡ç¥¨ã€‚
        
        Args:
            stock_list: è‚¡ç¥¨åˆ—è¡¨ï¼ˆNone åˆ™è‡ªåŠ¨è·å–ï¼‰
            progress_callback: è¿›åº¦å›è°ƒ (current, total, code)
            batch_log_size: æ‰¹é‡æ—¥å¿—é—´éš”
        
        Returns:
            ç»Ÿè®¡ç»“æœå­—å…¸
        """
        # ç¡®ä¿æœ‰å¯ç”¨èŠ‚ç‚¹
        if self.node_manager.get_available_count() == 0:
            self.test_nodes()
        
        if self.node_manager.get_available_count() == 0:
            raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„ TDX èŠ‚ç‚¹")
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        if stock_list is None:
            stock_list = self.get_all_stock_codes()
        
        total = len(stock_list)
        available_count = self.node_manager.get_available_count()
        effective_workers = min(self.max_workers, available_count * 3, total)
        
        # åˆå§‹åŒ–ç»Ÿè®¡
        self._stats = {
            'total': total,
            'success': 0,
            'skip': 0,
            'fail': 0,
            'start_time': datetime.now(),
            'end_time': None,
            'total_records': 0
        }
        
        self.logger.info("=" * 70)
        self.logger.info(f"ğŸš€ å¼€å§‹ä¸‹è½½ {total} åªè‚¡ç¥¨")
        self.logger.info(f"   å·¥ä½œçº¿ç¨‹: {effective_workers} | å¯ç”¨èŠ‚ç‚¹: {available_count}")
        self.logger.info(f"   å­˜å‚¨è·¯å¾„: {self.parquet_dir}")
        self.logger.info("=" * 70)
        
        # ä»»åŠ¡åˆ†é…ï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
        tasks_per_worker = [[] for _ in range(effective_workers)]
        for idx, stock in enumerate(stock_list):
            tasks_per_worker[idx % effective_workers].append(stock)
        
        completed = 0
        
        with ThreadPoolExecutor(max_workers=effective_workers) as executor:
            futures = {
                executor.submit(self._download_worker, tasks, wid): wid
                for wid, tasks in enumerate(tasks_per_worker) if tasks
            }
            
            # ============================================================
            # ã€CRITICAL FIXã€‘as_completed å®æ—¶è¿›åº¦åˆ·æ–°
            # ============================================================
            for future in as_completed(futures):
                worker_id = futures[future]
                
                try:
                    results = future.result()
                    
                    for result in results:
                        completed += 1
                        
                        # æ›´æ–°ç»Ÿè®¡ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
                        with self._stats_lock:
                            if result.success:
                                if result.records > 0:
                                    self._stats['success'] += 1
                                    self._stats['total_records'] += result.records
                                else:
                                    self._stats['skip'] += 1
                            else:
                                self._stats['fail'] += 1
                        
                        # å›è°ƒ
                        if progress_callback:
                            progress_callback(completed, total, result.code)
                        
                        # æ‰¹é‡æ—¥å¿—ï¼ˆå‡å°‘ I/Oï¼‰
                        if completed % batch_log_size == 0 or completed == total:
                            elapsed = (datetime.now() - self._stats['start_time']).total_seconds()
                            speed = completed / elapsed if elapsed > 0 else 0
                            eta = (total - completed) / speed if speed > 0 else 0

                            self.logger.info(
                                f"ğŸ“Š è¿›åº¦: {completed}/{total} ({completed/total*100:.1f}%) | "
                                f"æˆåŠŸ: {self._stats['success']} | "
                                f"è·³è¿‡: {self._stats['skip']} | "
                                f"å¤±è´¥: {self._stats['fail']} | "
                                f"é€Ÿåº¦: {speed:.1f}/s | "
                                f"å‰©ä½™: {eta:.0f}s"
                            )
                            sys.stdout.flush()
                
                except Exception as e:
                    self.logger.error(f"Worker-{worker_id} å¼‚å¸¸: {e}")
        
        self._stats['end_time'] = datetime.now()
        elapsed = (self._stats['end_time'] - self._stats['start_time']).total_seconds()
        
        self.logger.info("=" * 70)
        self.logger.info("âœ… ä¸‹è½½å®Œæˆ!")
        self.logger.info(
            f"   æ€»è®¡: {total} | æˆåŠŸ: {self._stats['success']} | "
            f"è·³è¿‡: {self._stats['skip']} | å¤±è´¥: {self._stats['fail']}"
        )
        self.logger.info(f"   æ€»è®°å½•æ•°: {self._stats['total_records']:,}")
        self.logger.info(f"   è€—æ—¶: {elapsed:.1f}s | å¹³å‡é€Ÿåº¦: {total/elapsed:.1f} åª/ç§’")
        self.logger.info("=" * 70)
        
        return self._stats.copy()
    
    def download_single(
        self,
        code: str,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None
    ) -> Optional[pd.DataFrame]:
        """ä¸‹è½½å•åªè‚¡ç¥¨ï¼ˆä¾¿æ·æ–¹æ³•ï¼‰"""
        if self.node_manager.get_available_count() == 0:
            self.test_nodes()
        
        # åˆ¤æ–­å¸‚åœº
        if any(code.startswith(p) for p in self.VALID_PREFIXES_SZ):
            market = 0
        elif any(code.startswith(p) for p in self.VALID_PREFIXES_SH):
            market = 1
        else:
            raise ValueError(f"æ— æ³•è¯†åˆ«çš„è‚¡ç¥¨ä»£ç : {code}")
        
        node = self.node_manager.get_node_by_index(0)
        if not node:
            raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„ TDX èŠ‚ç‚¹")
        
        api = TdxHq_API()
        try:
            if not api.connect(node.host, node.port, time_out=self.timeout):
                raise ConnectionError(f"æ— æ³•è¿æ¥åˆ°èŠ‚ç‚¹ {node.name}")
            
            start = pd.to_datetime(start_date) if start_date else None
            
            df, _ = self._download_stock_data(api, market, code, start)
            
            if df is not None and not df.empty:
                # Sanitize
                df = self.sanitizer.sanitize_dataframe(df)
                
                # æ—¥æœŸè¿‡æ»¤
                if end_date:
                    df = df[df.index <= pd.Timestamp(end_date)]
            
            return df
        
        finally:
            try:
                api.disconnect()
            except Exception as e:
                pass


# ============================================================================
# å¯¼å‡º
# ============================================================================

__all__ = [
    'TdxParallelDownloader',
    'TdxNodeManager',
    'AsyncLogHandler',
    'DownloadResult',
    'PYTDX_AVAILABLE',
]


# ============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================================================

if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(levelname)-8s | %(message)s',
        datefmt='%H:%M:%S'
    )
    
    print("\n" + "=" * 70)
    print("TDX å¹¶è¡Œæ•°æ®é‡‡é›†å™¨ v2.0.1ï¼ˆå·¥ä¸šçº§é‡æ„ç‰ˆï¼‰")
    print("=" * 70)
    
    # åˆå§‹åŒ–ç»„ä»¶
    storage = ColumnarStorageManager(base_dir="./data")
    sanitizer = DataSanitizer()
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = TdxParallelDownloader(
        storage_manager=storage,
        data_sanitizer=sanitizer,
        max_workers=15,
        timeout=5.0,
        enable_adjust=True,
        enable_async_log=True  # é«˜å¹¶å‘å¿…é¡»
    )
    
    # æµ‹è¯•èŠ‚ç‚¹
    print("\n[1/3] æµ‹è¯• TDX èŠ‚ç‚¹...")
    available = downloader.test_nodes()
    print(f"âœ“ å¯ç”¨èŠ‚ç‚¹: {len(available)}")
    
    # å•è‚¡ç¥¨æµ‹è¯•
    print("\n[2/3] å•è‚¡ç¥¨ä¸‹è½½æµ‹è¯•...")
    df = downloader.download_single("000001", start_date="2024-01-01")
    if df is not None:
        print(f"âœ“ 000001 ä¸‹è½½æˆåŠŸ: {len(df)} æ¡è®°å½•")
        print(df.tail(3))
    
    # æ‰¹é‡æµ‹è¯•
    print("\n[3/3] æ‰¹é‡ä¸‹è½½æµ‹è¯•...")
    test_codes = ['000001', '000002', '600000', '600036']
    stock_list = []
    for code in test_codes:
        if code.startswith(('00', '30')):
            stock_list.append((0, code))
        else:
            stock_list.append((1, code))
    
    stats = downloader.download_all_stocks(stock_list)
    print(f"âœ“ æ‰¹é‡ä¸‹è½½å®Œæˆ: æˆåŠŸ {stats['success']}, å¤±è´¥ {stats['fail']}")
    
    print("\n" + "=" * 70)
    print("æµ‹è¯•å®Œæˆ!")
    print("=" * 70)
